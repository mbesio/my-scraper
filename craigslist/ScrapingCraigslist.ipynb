{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/web-scraping-craigslist-a-complete-tutorial-c41cea4f4981\n",
    "#Importing all of the required packages\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import re\n",
    "from random import randint #avoid throttling by not sending too many requests one after the other\n",
    "from warnings import warn\n",
    "from time import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Url that we want to scrape\n",
    "url = \"https://sfbay.craigslist.org/search/sfc/apa?hasPic=1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get(url)\n",
    "page_soup = soup(response.text, \"html.parser\")\n",
    "\n",
    "#get the html container of the housing posts\n",
    "posts = page_soup.find_all('li', class_= 'result-row')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'potrero hill'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's get the neighbourhood\n",
    "hood = posts[0].find('span', class_ = 'result-hood')\n",
    "hood.text.strip().replace('(','').replace(')','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'$3871'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's get the price\n",
    "price = posts[0].find('span', class_ = 'result-price')\n",
    "price.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    }
   ],
   "source": [
    "#Let's get bed, bath and sqft\n",
    "bedding = posts[21].find('span', class_ = 'housing')\n",
    "\n",
    "\n",
    "if bedding is None:\n",
    "    number_bedroom = ''\n",
    "    sqft = ''\n",
    "else:\n",
    "    test = bedding.text.strip().split(\" \")\n",
    "    testlist = ' '.join(test).split()\n",
    "    \n",
    "    if len(testlist) == 4:\n",
    "        number_bedroom = testlist[0]\n",
    "        sqft = testlist[2]\n",
    "    \n",
    "    elif testlist[0][-2:] == 'br':\n",
    "        number_bedroom = testlist[0]\n",
    "        sqft = ''\n",
    "    \n",
    "    elif testlist[0][-3:] == 'ft2':\n",
    "        number_bedroom = ''\n",
    "        sqft = testlist[0]\n",
    "            \n",
    "#test = bedding.text.strip().split(\" \")\n",
    "#testlist = ' '.join(test).split()\n",
    "#number_bedroom = testlist[0]\n",
    "#number_bathroom = testlist[1]\n",
    "#sqft = testlist[2]\n",
    "#print(test)\n",
    "#print(testlist)\n",
    "print(number_bedroom, sqft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the loop on all the listings and on all the pages\n",
    "results_num = page_soup.find('div', class_= 'search-legend')\n",
    "results_total = int(results_num.find('span', class_='totalcount').text)\n",
    "pages = np.arange(0, results_total+1, 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing some of the arrays we will need once we loop\n",
    "iterations = 0\n",
    "\n",
    "post_timing = []\n",
    "post_hoods = []\n",
    "post_title_texts = []\n",
    "bedroom_counts = []\n",
    "sqfts = []\n",
    "post_links = []\n",
    "post_prices = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 scraped successfully!\n",
      "Page 2 scraped successfully!\n",
      "Page 3 scraped successfully!\n",
      "Page 4 scraped successfully!\n",
      "Page 5 scraped successfully!\n",
      "Page 6 scraped successfully!\n",
      "Page 7 scraped successfully!\n",
      "Page 8 scraped successfully!\n",
      "Page 9 scraped successfully!\n",
      "Page 10 scraped successfully!\n",
      "Page 11 scraped successfully!\n",
      "Page 12 scraped successfully!\n",
      "Page 13 scraped successfully!\n",
      "Page 14 scraped successfully!\n",
      "Page 15 scraped successfully!\n",
      "Page 16 scraped successfully!\n",
      "Page 17 scraped successfully!\n",
      "Page 18 scraped successfully!\n",
      "Page 19 scraped successfully!\n",
      "Page 20 scraped successfully!\n",
      "Page 21 scraped successfully!\n",
      "Page 22 scraped successfully!\n",
      "Page 23 scraped successfully!\n",
      "Page 24 scraped successfully!\n",
      "Page 25 scraped successfully!\n",
      "Page 26 scraped successfully!\n",
      "\n",
      "\n",
      "Scrape complete!\n"
     ]
    }
   ],
   "source": [
    "for page in pages:\n",
    "\n",
    "    #get requests\n",
    "    response = get(\"https://sfbay.craigslist.org/search/sfc/apt?\"\n",
    "                   + \"s=\" #the parameter for defining the page number\n",
    "                   + str(page) #the page number in the pages array from earlier\n",
    "                   + \"&hasPic=1\"\n",
    "                   + \"&availabilityMode=0\")\n",
    "\n",
    "\n",
    "    sleep(randint(1,5))\n",
    "\n",
    "    #throw warning for status codes that are not 200\n",
    "    if response.status_code != 200:\n",
    "        warn('Request: {}; Status code: {}'.format(requests, response.status_code))\n",
    "\n",
    "    #define the html text\n",
    "    page_html = soup(response.text, 'html.parser')\n",
    "\n",
    "    #define the posts\n",
    "    posts = page_html.find_all('li', class_= 'result-row')\n",
    "\n",
    "    for post in posts:\n",
    "        if post.find('span', class_ = 'result-hood') is not None:\n",
    "            #posting date\n",
    "            #grab the datetime element 0 for date and 1 for time\n",
    "            post_datetime = post.find('time', class_= 'result-date')['datetime']\n",
    "            post_timing.append(post_datetime)\n",
    "\n",
    "            #neighborhoods\n",
    "            post_hood = post.find('span', class_= 'result-hood').text\n",
    "            post_hoods.append(post_hood)\n",
    "\n",
    "            #title text\n",
    "            post_title = post.find('a', class_='result-title hdrlnk')\n",
    "            post_title_text = post_title.text\n",
    "            post_title_texts.append(post_title_text)\n",
    "\n",
    "            #post link\n",
    "            post_link = post_title['href']\n",
    "            post_links.append(post_link)\n",
    "\n",
    "            #removes the \\n whitespace from each side, removes the currency symbol, and turns it into an int\n",
    "            post_price = int(post.a.text.strip().replace(\"$\", \"\"))\n",
    "            post_prices.append(post_price)\n",
    "\n",
    "            if post.find('span', class_ = 'housing') is not None:\n",
    "\n",
    "                #if the first element is accidentally square footage\n",
    "                if 'ft2' in post.find('span', class_ = 'housing').text.split()[0]:\n",
    "\n",
    "                    #make bedroom nan\n",
    "                    bedroom_count = np.nan\n",
    "                    bedroom_counts.append(bedroom_count)\n",
    "\n",
    "                    #make sqft the first element\n",
    "                    sqft = int(post.find('span', class_ = 'housing').text.split()[0][:-3])\n",
    "                    sqfts.append(sqft)\n",
    "\n",
    "                #if the length of the housing details element is more than 2\n",
    "                elif len(post.find('span', class_ = 'housing').text.split()) > 2:\n",
    "\n",
    "                    #therefore element 0 will be bedroom count\n",
    "                    bedroom_count = post.find('span', class_ = 'housing').text.replace(\"br\", \"\").split()[0]\n",
    "                    bedroom_counts.append(bedroom_count)\n",
    "\n",
    "                    #and sqft will be number 3, so set these here and append\n",
    "                    sqft = int(post.find('span', class_ = 'housing').text.split()[2][:-3])\n",
    "                    sqfts.append(sqft)\n",
    "\n",
    "                #if there is num bedrooms but no sqft\n",
    "                elif len(post.find('span', class_ = 'housing').text.split()) == 2:\n",
    "\n",
    "                    #therefore element 0 will be bedroom count\n",
    "                    bedroom_count = post.find('span', class_ = 'housing').text.replace(\"br\", \"\").split()[0]\n",
    "                    bedroom_counts.append(bedroom_count)\n",
    "\n",
    "                    #and sqft will be number 3, so set these here and append\n",
    "                    sqft = np.nan\n",
    "                    sqfts.append(sqft)\n",
    "\n",
    "                else:\n",
    "                    bedroom_count = np.nan\n",
    "                    bedroom_counts.append(bedroom_count)\n",
    "\n",
    "                    sqft = np.nan\n",
    "                    sqfts.append(sqft)\n",
    "\n",
    "                #if none of those conditions catch, make bedroom nan, this won't be needed\n",
    "            else:\n",
    "                bedroom_count = np.nan\n",
    "                bedroom_counts.append(bedroom_count)\n",
    "\n",
    "                sqft = np.nan\n",
    "                sqfts.append(sqft)\n",
    "                #    bedroom_counts.append(bedroom_count)\n",
    "\n",
    "                #    sqft = np.nan\n",
    "                #    sqfts.append(sqft)\n",
    "\n",
    "    iterations += 1\n",
    "    print(\"Page \" + str(iterations) + \" scraped successfully!\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Scrape complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "eb_apts = pd.DataFrame({'posted': post_timing,\n",
    "                       'neighborhood': post_hoods,\n",
    "                       'post title': post_title_texts,\n",
    "                       'number bedrooms': bedroom_counts,\n",
    "                        'sqft': sqfts,\n",
    "                        'URL': post_links,\n",
    "                       'price': post_prices})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2868 entries, 0 to 2867\n",
      "Data columns (total 7 columns):\n",
      "posted             2868 non-null object\n",
      "neighborhood       2868 non-null object\n",
      "post title         2868 non-null object\n",
      "number bedrooms    0 non-null float64\n",
      "sqft               0 non-null float64\n",
      "URL                2868 non-null object\n",
      "price              2868 non-null int64\n",
      "dtypes: float64(2), int64(1), object(4)\n",
      "memory usage: 156.9+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posted</th>\n",
       "      <th>neighborhood</th>\n",
       "      <th>post title</th>\n",
       "      <th>number bedrooms</th>\n",
       "      <th>sqft</th>\n",
       "      <th>URL</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-04-25 18:30</td>\n",
       "      <td>(san rafael)</td>\n",
       "      <td>Subaru Outback 2008 Fender drivers side plus f...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://sfbay.craigslist.org/nby/pts/d/san-raf...</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-04-25 18:30</td>\n",
       "      <td>(san rafael)</td>\n",
       "      <td>Subaru Outback 2008 Fender drivers side plus f...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://sfbay.craigslist.org/nby/pts/d/san-raf...</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-04-25 18:30</td>\n",
       "      <td>(cupertino)</td>\n",
       "      <td>3TB Seagate SkyHawk Surveillance Hard Drive ST...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://sfbay.craigslist.org/sby/sop/d/sunnyva...</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-04-25 18:30</td>\n",
       "      <td>(+ CAR EXPO AUTO CENTER)</td>\n",
       "      <td>2013 Honda Accord EX-L v6 -TOP $$$ FOR YOUR TR...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://sfbay.craigslist.org/eby/ctd/d/sacrame...</td>\n",
       "      <td>10979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-04-25 18:30</td>\n",
       "      <td>(richmond / point / annex)</td>\n",
       "      <td>Cuisinart  Electric Knife, $18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://sfbay.craigslist.org/eby/for/d/cuisina...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2019-04-25 18:30</td>\n",
       "      <td>(milpitas)</td>\n",
       "      <td>White table</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://sfbay.craigslist.org/sby/fuo/d/san-jos...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2019-04-25 18:30</td>\n",
       "      <td>(Tracy)</td>\n",
       "      <td>Fisher Marine aluminum Honda outboard</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://sfbay.craigslist.org/sby/boa/d/banta-f...</td>\n",
       "      <td>2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2019-04-25 18:30</td>\n",
       "      <td>(san francisco)</td>\n",
       "      <td>Large Mirror 5'x4'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://sfbay.craigslist.org/sfc/hsh/d/san-fra...</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2019-04-25 18:30</td>\n",
       "      <td>(santa cruz)</td>\n",
       "      <td>Men's O'Neill Wetsuit Heat</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://sfbay.craigslist.org/scz/spo/d/santa-c...</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2019-04-25 18:30</td>\n",
       "      <td>(napa county)</td>\n",
       "      <td>Beanie Babies - PRISTINE WITH TAGS!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://sfbay.craigslist.org/nby/clt/d/napa-be...</td>\n",
       "      <td>2500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             posted                 neighborhood  \\\n",
       "0  2019-04-25 18:30                 (san rafael)   \n",
       "1  2019-04-25 18:30                 (san rafael)   \n",
       "2  2019-04-25 18:30                  (cupertino)   \n",
       "3  2019-04-25 18:30     (+ CAR EXPO AUTO CENTER)   \n",
       "4  2019-04-25 18:30   (richmond / point / annex)   \n",
       "5  2019-04-25 18:30                   (milpitas)   \n",
       "6  2019-04-25 18:30                      (Tracy)   \n",
       "7  2019-04-25 18:30              (san francisco)   \n",
       "8  2019-04-25 18:30                 (santa cruz)   \n",
       "9  2019-04-25 18:30                (napa county)   \n",
       "\n",
       "                                          post title  number bedrooms  sqft  \\\n",
       "0  Subaru Outback 2008 Fender drivers side plus f...              NaN   NaN   \n",
       "1  Subaru Outback 2008 Fender drivers side plus f...              NaN   NaN   \n",
       "2  3TB Seagate SkyHawk Surveillance Hard Drive ST...              NaN   NaN   \n",
       "3  2013 Honda Accord EX-L v6 -TOP $$$ FOR YOUR TR...              NaN   NaN   \n",
       "4                     Cuisinart  Electric Knife, $18              NaN   NaN   \n",
       "5                                        White table              NaN   NaN   \n",
       "6              Fisher Marine aluminum Honda outboard              NaN   NaN   \n",
       "7                                 Large Mirror 5'x4'              NaN   NaN   \n",
       "8                         Men's O'Neill Wetsuit Heat              NaN   NaN   \n",
       "9                Beanie Babies - PRISTINE WITH TAGS!              NaN   NaN   \n",
       "\n",
       "                                                 URL  price  \n",
       "0  https://sfbay.craigslist.org/nby/pts/d/san-raf...    175  \n",
       "1  https://sfbay.craigslist.org/nby/pts/d/san-raf...    175  \n",
       "2  https://sfbay.craigslist.org/sby/sop/d/sunnyva...     65  \n",
       "3  https://sfbay.craigslist.org/eby/ctd/d/sacrame...  10979  \n",
       "4  https://sfbay.craigslist.org/eby/for/d/cuisina...     18  \n",
       "5  https://sfbay.craigslist.org/sby/fuo/d/san-jos...      0  \n",
       "6  https://sfbay.craigslist.org/sby/boa/d/banta-f...   2500  \n",
       "7  https://sfbay.craigslist.org/sfc/hsh/d/san-fra...    100  \n",
       "8  https://sfbay.craigslist.org/scz/spo/d/santa-c...    200  \n",
       "9  https://sfbay.craigslist.org/nby/clt/d/napa-be...   2500  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(eb_apts.info())\n",
    "eb_apts.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
